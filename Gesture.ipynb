{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "num_frames = 0\n",
    "element = 'A'\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            #cv2.imshow(\"Sign Detection\",frame_copy)\n",
    "         \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        \n",
    "        # Checking if hand is actually detected by counting number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                #cv2.imwrite(r\"D:\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" + str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                cv2.imwrite(\"data2/train/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            elif num_imgs_taken > 300 and num_imgs_taken <= 400:\n",
    "                cv2.imwrite(\"data2/test/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "#             else:\n",
    "#                 break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"Hand Sign Recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# train_path = 'dataset/train'\n",
    "# test_path = 'dataset/test'\n",
    "\n",
    "# train_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, \n",
    "#                                                                                              target_size=(64,64),\n",
    "#                                                                                              class_mode='categorical', \n",
    "#                                                                                              batch_size=10,shuffle=True)\n",
    "# test_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, \n",
    "#                                                                                              target_size=(64,64), \n",
    "#                                                                                              class_mode='categorical', \n",
    "#                                                                                              batch_size=10, shuffle=True)\n",
    "\n",
    "# imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "# #Plotting the images...\n",
    "# def plotImages(images_arr):\n",
    "#     fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "#     axes = axes.flatten()\n",
    "#     for img, ax in zip( images_arr, axes):\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         ax.imshow(img)\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotImages(imgs)\n",
    "# print(imgs.shape)\n",
    "# print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# # model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "# # model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(64,activation =\"relu\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# # model.add(Dense(128,activation =\"relu\"))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # # model.add(Dense(256,activation =\"relu\"))\n",
    "# # # model.add(Dropout(0.5))\n",
    "# model.add(Dense(10,activation =\"softmax\"))\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import ReduceLROnPlateau\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\"sign_model.h5\",\n",
    "#                              monitor=\"val_loss\",\n",
    "#                              mode=\"min\",\n",
    "#                              save_best_only = True,\n",
    "#                              verbose=1)\n",
    "\n",
    "# earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "#                           min_delta = 0, \n",
    "#                           patience = 3,\n",
    "#                           verbose = 1,\n",
    "#                           restore_best_weights = True)\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# nb_train_samples = 3010\n",
    "# nb_validation_samples = 1000\n",
    "# batch_size=8\n",
    "\n",
    "# history2 = model.fit_generator(train_batches, epochs=50,\n",
    "#                                callbacks=[reduce_lr, earlystop],\n",
    "#                                steps_per_epoch = nb_train_samples // batch_size,\n",
    "#                                validation_data = test_batches,\n",
    "#                                validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 61, 61, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               3686528   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 96)                12384     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 3,716,443\n",
      "Trainable params: 3,716,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 12845 images belonging to 27 classes.\n",
      "Found 4268 images belonging to 27 classes.\n",
      "Epoch 1/5\n",
      "12841/12841 [==============================] - 2764s 215ms/step - loss: 0.4991 - acc: 0.8404 - val_loss: 0.0062 - val_acc: 0.9984\n",
      "Epoch 2/5\n",
      "12841/12841 [==============================] - 2214s 172ms/step - loss: 0.1196 - acc: 0.9634 - val_loss: 0.0042 - val_acc: 0.9988\n",
      "Epoch 3/5\n",
      "12841/12841 [==============================] - 2580s 201ms/step - loss: 0.0872 - acc: 0.9738 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "12841/12841 [==============================] - 2801s 218ms/step - loss: 0.0686 - acc: 0.9800 - val_loss: 0.0034 - val_acc: 0.9986\n",
      "Epoch 5/5\n",
      "12841/12841 [==============================] - 2707s 211ms/step - loss: 0.0612 - acc: 0.9829 - val_loss: 0.0035 - val_acc: 0.9991\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout\n",
    "import os\n",
    "\n",
    "sz = 128\n",
    "# Step 1 - Building the CNN\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "#classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=96, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
    "\n",
    "\n",
    "# Step 2 - Preparing the train/test data and training the model\n",
    "classifier.summary()\n",
    "# Code copied from - https://keras.io/preprocessing/image/\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/train',\n",
    "                                                 target_size=(sz, sz),\n",
    "                                                 batch_size=10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test',\n",
    "                                            target_size=(sz , sz),\n",
    "                                            batch_size=10,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical') \n",
    "\n",
    "batch_size = 32\n",
    "training_history = classifier.fit_generator(training_set,\n",
    "                                            steps_per_epoch=12841, # No of images in training set\n",
    "                                            epochs=5,\n",
    "                                            validation_data=test_set,\n",
    "                                            validation_steps=4268)# No of images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Char_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "# model.save('best_model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(\"Char_model.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "plt.plot(model.metrics_names[0],model.metrics_names[1])\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {0:'One',1:'Ten',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'blank', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "Z\n",
      "Z\n",
      "Z\n",
      "O\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "Z\n",
      "Z\n",
      "Z\n",
      "Z\n",
      "Z\n",
      "Z\n",
      "K\n",
      "Y\n",
      "Y\n",
      "Z\n",
      "Z\n",
      "Z\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "T\n",
      "N\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "Z\n",
      "D\n",
      "F\n",
      "F\n",
      "F\n",
      "Z\n",
      "N\n",
      "X\n",
      "X\n",
      "O\n",
      "blank\n",
      "O\n",
      "O\n",
      "blank\n",
      "H\n",
      "J\n",
      "C\n",
      "C\n",
      "O\n",
      "C\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "Z\n",
      "H\n",
      "G\n",
      "G\n",
      "D\n",
      "G\n",
      "G\n",
      "G\n",
      "D\n",
      "D\n",
      "Z\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "W\n",
      "W\n",
      "W\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "V\n",
      "V\n",
      "V\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "D\n",
      "D\n",
      "R\n",
      "R\n",
      "B\n",
      "B\n",
      "R\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "W\n",
      "V\n",
      "B\n",
      "B\n",
      "B\n",
      "U\n",
      "B\n",
      "B\n",
      "B\n",
      "U\n",
      "U\n",
      "U\n",
      "U\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "D\n",
      "B\n",
      "B\n",
      "B\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "H\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "blank\n",
      "Z\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "from string import ascii_uppercase\n",
    "\n",
    "\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "prediction={}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "model = keras.models.load_model(\"Char_model.h5\")\n",
    "minValue = 25\n",
    "# print(model.summary())\n",
    "preds = ['blank']\n",
    "for i in ascii_uppercase:\n",
    "    preds.append(i)\n",
    "\n",
    "preds[0],preds[1] = preds[1],preds[0]\n",
    "print(preds)\n",
    "\n",
    "num_frames = 0\n",
    "temp_res = []\n",
    "final_res = \"\"\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    # Simulating mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2.rectangle(frame, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)    # Extracting the ROI\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "#   roi = cv2.resize(roi, (128,128))\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "    # #blur = cv2.bilateralFilter(roi,9,75,75)\n",
    "\n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    test_image = cv2.resize(test_image, (128,128) )\n",
    "    \n",
    "    test_predict = test_image.reshape(1,128,128,1)\n",
    "    \n",
    "    if num_frames<30:\n",
    "        num_frames += 1\n",
    "        res = np.argmax(model.predict(test_predict, 1, verbose = 0), axis=1)\n",
    "        temp_res.append(preds[res[0]])\n",
    "    \n",
    "    if num_frames >=30:\n",
    "        num_frames = 0\n",
    "        final_res = max(set(temp_res), key = temp_res.count)\n",
    "        temp_res = []\n",
    "        print(final_res)\n",
    "    \n",
    "    \n",
    "#     print(res[0])\n",
    "#     print(preds[res[0]])\n",
    "    test_image = cv2.resize(test_image, (256,256) )\n",
    "    cv2.putText(test_image, final_res , (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow(\"test\", test_image)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "# print(model.predict(test_predict, 1, verbose = 0))\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(model.predict(test_predict, 1, verbose = 0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 61, 61, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               3686528   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 96)                12384     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 3,716,443\n",
      "Trainable params: 3,716,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 20,\n",
       " 'B': 20,\n",
       " 'C': 20,\n",
       " 'D': 20,\n",
       " 'E': 20,\n",
       " 'F': 20,\n",
       " 'G': 20,\n",
       " 'H': 20,\n",
       " 'I': 20,\n",
       " 'J': 20,\n",
       " 'K': 20,\n",
       " 'L': 20,\n",
       " 'M': 20,\n",
       " 'N': 20,\n",
       " 'O': 20,\n",
       " 'P': 20,\n",
       " 'Q': 20,\n",
       " 'R': 20,\n",
       " 'S': 20,\n",
       " 'T': 20,\n",
       " 'U': 20,\n",
       " 'V': 20,\n",
       " 'W': 20,\n",
       " 'X': 20,\n",
       " 'Y': 20,\n",
       " 'Z': 20}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result)\n",
    "from string import ascii_uppercase\n",
    "prediction={}\n",
    "inde=1\n",
    "for i in ascii_uppercase:\n",
    "    prediction[i] = result[0]\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05b57464ba20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'temp_res' is not defined"
     ]
    }
   ],
   "source": [
    "print(temp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Application...\n"
     ]
    },
    {
     "ename": "HunspellFilePathError",
     "evalue": "File 'c:/usr/share/hunspell/en_US.dic.aff' not found or accessible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHunspellFilePathError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5200934b7c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting Application...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m \u001b[0mpba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mApplication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[0mpba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5200934b7c21>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhunspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHunspell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/usr/share/hunspell/en_US.dic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/usr/share/hunspell/en_US.aff'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\hci\\lib\\site-packages\\hunspell\\hunspell.pyx\u001b[0m in \u001b[0;36mhunspell.hunspell.HunspellWrap.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\hci\\lib\\site-packages\\hunspell\\hunspell.pyx\u001b[0m in \u001b[0;36mhunspell.hunspell.HunspellWrap._create_hspell_inst\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mHunspellFilePathError\u001b[0m: File 'c:/usr/share/hunspell/en_US.dic.aff' not found or accessible"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import operator\n",
    "import time\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import hunspell\n",
    "from string import ascii_uppercase\n",
    "\n",
    "class Application:\n",
    "    def __init__(self):\n",
    "        self.directory = ''\n",
    "        self.hs = hunspell.Hunspell('C:/Users/goelm/Anaconda3/envs/hci/Lib/site-packages/hunspell/dictionaries/en_US.txt', 'C:/Users/goelm/Anaconda3/envs/hci/Lib/site-packages/hunspell/dictionaries/en_US.aff')\n",
    "        self.vs = cv2.VideoCapture(0)\n",
    "        self.current_image = None\n",
    "        self.current_image2 = None\n",
    "        \n",
    "        self.json_file = open(self.directory+\"model-bw.json\", \"r\")\n",
    "        self.model_json = self.json_file.read()\n",
    "        self.json_file.close()\n",
    "        self.loaded_model = model_from_json(self.model_json)\n",
    "        self.loaded_model.load_weights(self.directory+\"model-bw.h5\")\n",
    "\n",
    "        self.json_file_dru = open(self.directory+\"model-bw_dru.json\" , \"r\")\n",
    "        self.model_json_dru = self.json_file_dru.read()\n",
    "        self.json_file_dru.close()\n",
    "        self.loaded_model_dru = model_from_json(self.model_json_dru)\n",
    "        self.loaded_model_dru.load_weights(\"model-bw_dru.h5\")\n",
    "\n",
    "        self.json_file_tkdi = open(self.directory+\"model-bw_tkdi.json\" , \"r\")\n",
    "        self.model_json_tkdi = self.json_file_tkdi.read()\n",
    "        self.json_file_tkdi.close()\n",
    "        self.loaded_model_tkdi = model_from_json(self.model_json_tkdi)\n",
    "        self.loaded_model_tkdi.load_weights(self.directory+\"model-bw_tkdi.h5\")\n",
    "\n",
    "        self.json_file_smn = open(self.directory+\"model-bw_smn.json\" , \"r\")\n",
    "        self.model_json_smn = self.json_file_smn.read()\n",
    "        self.json_file_smn.close()\n",
    "        self.loaded_model_smn = model_from_json(self.model_json_smn)\n",
    "        self.loaded_model_smn.load_weights(self.directory+\"model-bw_smn.h5\")\n",
    "        \n",
    "        self.ct = {}\n",
    "        self.ct['blank'] = 0\n",
    "        self.blank_flag = 0\n",
    "        for i in ascii_uppercase:\n",
    "          self.ct[i] = 0\n",
    "        print(\"Loaded model from disk\")\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Sign language to Text Converter\")\n",
    "        self.root.protocol('WM_DELETE_WINDOW', self.destructor)\n",
    "        self.root.geometry(\"900x1100\")\n",
    "        self.panel = tk.Label(self.root)\n",
    "        self.panel.place(x = 135, y = 10, width = 640, height = 640)\n",
    "        self.panel2 = tk.Label(self.root) # initialize image panel\n",
    "        self.panel2.place(x = 460, y = 95, width = 310, height = 310)\n",
    "        \n",
    "        self.T = tk.Label(self.root)\n",
    "        self.T.place(x=31,y = 17)\n",
    "        self.T.config(text = \"Sign Language to Text\",font=(\"courier\",40,\"bold\"))\n",
    "        self.panel3 = tk.Label(self.root) # Current SYmbol\n",
    "        self.panel3.place(x = 500,y=640)\n",
    "        self.T1 = tk.Label(self.root)\n",
    "        self.T1.place(x = 10,y = 640)\n",
    "        self.T1.config(text=\"Character :\",font=(\"Courier\",40,\"bold\"))\n",
    "        self.panel4 = tk.Label(self.root) # Word\n",
    "        self.panel4.place(x = 220,y=700)\n",
    "        self.T2 = tk.Label(self.root)\n",
    "        self.T2.place(x = 10,y = 700)\n",
    "        self.T2.config(text =\"Word :\",font=(\"Courier\",40,\"bold\"))\n",
    "        self.panel5 = tk.Label(self.root) # Sentence\n",
    "        self.panel5.place(x = 350,y=760)\n",
    "        self.T3 = tk.Label(self.root)\n",
    "        self.T3.place(x = 10,y = 760)\n",
    "        self.T3.config(text =\"Sentence :\",font=(\"Courier\",40,\"bold\"))\n",
    "\n",
    "        self.T4 = tk.Label(self.root)\n",
    "        self.T4.place(x = 250,y = 820)\n",
    "        self.T4.config(text = \"Suggestions\",fg=\"red\",font = (\"Courier\",40,\"bold\"))\n",
    "\n",
    "        self.btcall = tk.Button(self.root,command = self.action_call,height = 0,width = 0)\n",
    "        self.btcall.config(text = \"About\",font = (\"Courier\",14))\n",
    "        self.btcall.place(x = 825, y = 0)\n",
    "\n",
    "        self.bt1=tk.Button(self.root, command=self.action1,height = 0,width = 0)\n",
    "        self.bt1.place(x = 26,y=890)\n",
    "        #self.bt1.grid(padx = 10, pady = 10)\n",
    "        self.bt2=tk.Button(self.root, command=self.action2,height = 0,width = 0)\n",
    "        self.bt2.place(x = 325,y=890)\n",
    "        #self.panel3.place(x = 10,y=660)\n",
    "        # self.bt2.grid(row = 4, column = 1, columnspan = 1, padx = 10, pady = 10, sticky = tk.NW)\n",
    "        self.bt3=tk.Button(self.root, command=self.action3,height = 0,width = 0)\n",
    "        self.bt3.place(x = 625,y=890)\n",
    "        # self.bt3.grid(row = 4, column = 2, columnspan = 1, padx = 10, pady = 10, sticky = tk.NW)\n",
    "        self.bt4=tk.Button(self.root, command=self.action4,height = 0,width = 0)\n",
    "        self.bt4.place(x = 125,y=950)\n",
    "        # self.bt4.grid(row = bt1, column = 0, columnspan = 1, padx = 10, pady = 10, sticky = tk.N)\n",
    "        self.bt5=tk.Button(self.root, command=self.action5,height = 0,width = 0)\n",
    "        self.bt5.place(x = 425,y=950)\n",
    "        # self.bt5.grid(row = 5, column = 1, columnspan = 1, padx = 10, pady = 10, sticky = tk.N)\n",
    "        self.str=\"\"\n",
    "        self.word=\"\"\n",
    "        self.current_symbol=\"Empty\"\n",
    "        self.photo=\"Empty\"\n",
    "        self.video_loop()\n",
    "\n",
    "    def video_loop(self):\n",
    "        ok, frame = self.vs.read()\n",
    "        if ok:\n",
    "            cv2image = cv2.flip(frame, 1)\n",
    "            x1 = int(0.5*frame.shape[1])\n",
    "            y1 = 10\n",
    "            x2 = frame.shape[1]-10\n",
    "            y2 = int(0.5*frame.shape[1])\n",
    "            cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)\n",
    "            cv2image = cv2.cvtColor(cv2image, cv2.COLOR_BGR2RGBA)\n",
    "            self.current_image = Image.fromarray(cv2image)\n",
    "            imgtk = ImageTk.PhotoImage(image=self.current_image)\n",
    "            self.panel.imgtk = imgtk\n",
    "            self.panel.config(image=imgtk)\n",
    "            cv2image = cv2image[y1:y2, x1:x2]\n",
    "            gray = cv2.cvtColor(cv2image, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "            th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "            ret, res = cv2.threshold(th3, 70, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "            self.predict(res)\n",
    "            self.current_image2 = Image.fromarray(res)\n",
    "            imgtk = ImageTk.PhotoImage(image=self.current_image2)\n",
    "            self.panel2.imgtk = imgtk\n",
    "            self.panel2.config(image=imgtk)\n",
    "            self.panel3.config(text=self.current_symbol,font=(\"Courier\",50))\n",
    "            self.panel4.config(text=self.word,font=(\"Courier\",40))\n",
    "            self.panel5.config(text=self.str,font=(\"Courier\",40))\n",
    "            predicts=self.hs.suggest(self.word)\n",
    "            if(len(predicts) > 0):\n",
    "                self.bt1.config(text=predicts[0],font = (\"Courier\",20))\n",
    "            else:\n",
    "                self.bt1.config(text=\"\")\n",
    "            if(len(predicts) > 1):\n",
    "                self.bt2.config(text=predicts[1],font = (\"Courier\",20))\n",
    "            else:\n",
    "                self.bt2.config(text=\"\")\n",
    "            if(len(predicts) > 2):\n",
    "                self.bt3.config(text=predicts[2],font = (\"Courier\",20))\n",
    "            else:\n",
    "                self.bt3.config(text=\"\")\n",
    "            if(len(predicts) > 3):\n",
    "                self.bt4.config(text=predicts[3],font = (\"Courier\",20))\n",
    "            else:\n",
    "                self.bt4.config(text=\"\")\n",
    "            if(len(predicts) > 4):\n",
    "                self.bt4.config(text=predicts[4],font = (\"Courier\",20))\n",
    "            else:\n",
    "                self.bt4.config(text=\"\")                \n",
    "        self.root.after(30, self.video_loop)\n",
    "    def predict(self,test_image):\n",
    "        test_image = cv2.resize(test_image, (128,128))\n",
    "        result = self.loaded_model.predict(test_image.reshape(1, 128, 128, 1))\n",
    "        result_dru = self.loaded_model_dru.predict(test_image.reshape(1 , 128 , 128 , 1))\n",
    "        result_tkdi = self.loaded_model_tkdi.predict(test_image.reshape(1 , 128 , 128 , 1))\n",
    "        result_smn = self.loaded_model_smn.predict(test_image.reshape(1 , 128 , 128 , 1))\n",
    "        prediction={}\n",
    "        prediction['blank'] = result[0][0]\n",
    "        inde = 1\n",
    "        for i in ascii_uppercase:\n",
    "            prediction[i] = result[0][inde]\n",
    "            inde += 1\n",
    "        #LAYER 1\n",
    "        prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        self.current_symbol = prediction[0][0]\n",
    "        #LAYER 2\n",
    "        if(self.current_symbol == 'D' or self.current_symbol == 'R' or self.current_symbol == 'U'):\n",
    "        \tprediction = {}\n",
    "        \tprediction['D'] = result_dru[0][0]\n",
    "        \tprediction['R'] = result_dru[0][1]\n",
    "        \tprediction['U'] = result_dru[0][2]\n",
    "        \tprediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \tself.current_symbol = prediction[0][0]\n",
    "\n",
    "        if(self.current_symbol == 'D' or self.current_symbol == 'I' or self.current_symbol == 'K' or self.current_symbol == 'T'):\n",
    "        \tprediction = {}\n",
    "        \tprediction['D'] = result_tkdi[0][0]\n",
    "        \tprediction['I'] = result_tkdi[0][1]\n",
    "        \tprediction['K'] = result_tkdi[0][2]\n",
    "        \tprediction['T'] = result_tkdi[0][3]\n",
    "        \tprediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \tself.current_symbol = prediction[0][0]\n",
    "\n",
    "        if(self.current_symbol == 'M' or self.current_symbol == 'N' or self.current_symbol == 'S'):\n",
    "        \tprediction1 = {}\n",
    "        \tprediction1['M'] = result_smn[0][0]\n",
    "        \tprediction1['N'] = result_smn[0][1]\n",
    "        \tprediction1['S'] = result_smn[0][2]\n",
    "        \tprediction1 = sorted(prediction1.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \tif(prediction1[0][0] == 'S'):\n",
    "        \t\tself.current_symbol = prediction1[0][0]\n",
    "        \telse:\n",
    "        \t\tself.current_symbol = prediction[0][0]\n",
    "        if(self.current_symbol == 'blank'):\n",
    "            for i in ascii_uppercase:\n",
    "                self.ct[i] = 0\n",
    "        self.ct[self.current_symbol] += 1\n",
    "        if(self.ct[self.current_symbol] > 60):\n",
    "            for i in ascii_uppercase:\n",
    "                if i == self.current_symbol:\n",
    "                    continue\n",
    "                tmp = self.ct[self.current_symbol] - self.ct[i]\n",
    "                if tmp < 0:\n",
    "                    tmp *= -1\n",
    "                if tmp <= 20:\n",
    "                    self.ct['blank'] = 0\n",
    "                    for i in ascii_uppercase:\n",
    "                        self.ct[i] = 0\n",
    "                    return\n",
    "            self.ct['blank'] = 0\n",
    "            for i in ascii_uppercase:\n",
    "                self.ct[i] = 0\n",
    "            if self.current_symbol == 'blank':\n",
    "                if self.blank_flag == 0:\n",
    "                    self.blank_flag = 1\n",
    "                    if len(self.str) > 0:\n",
    "                        self.str += \" \"\n",
    "                    self.str += self.word\n",
    "                    self.word = \"\"\n",
    "            else:\n",
    "                if(len(self.str) > 16):\n",
    "                    self.str = \"\"\n",
    "                self.blank_flag = 0\n",
    "                self.word += self.current_symbol\n",
    "    def action1(self):\n",
    "    \tpredicts=self.hs.suggest(self.word)\n",
    "    \tif(len(predicts) > 0):\n",
    "            self.word=\"\"\n",
    "            self.str+=\" \"\n",
    "            self.str+=predicts[0]\n",
    "    def action2(self):\n",
    "    \tpredicts=self.hs.suggest(self.word)\n",
    "    \tif(len(predicts) > 1):\n",
    "            self.word=\"\"\n",
    "            self.str+=\" \"\n",
    "            self.str+=predicts[1]\n",
    "    def action3(self):\n",
    "    \tpredicts=self.hs.suggest(self.word)\n",
    "    \tif(len(predicts) > 2):\n",
    "            self.word=\"\"\n",
    "            self.str+=\" \"\n",
    "            self.str+=predicts[2]\n",
    "    def action4(self):\n",
    "    \tpredicts=self.hs.suggest(self.word)\n",
    "    \tif(len(predicts) > 3):\n",
    "            self.word=\"\"\n",
    "            self.str+=\" \"\n",
    "            self.str+=predicts[3]\n",
    "    def action5(self):\n",
    "    \tpredicts=self.hs.suggest(self.word)\n",
    "    \tif(len(predicts) > 4):\n",
    "            self.word=\"\"\n",
    "            self.str+=\" \"\n",
    "            self.str+=predicts[4]\n",
    "    def destructor(self):\n",
    "        print(\"Closing Application...\")\n",
    "        self.root.destroy()\n",
    "        self.vs.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def destructor1(self):\n",
    "        print(\"Closing Application...\")\n",
    "        self.root1.destroy()\n",
    "\n",
    "    def action_call(self) :\n",
    "        \n",
    "        self.root1 = tk.Toplevel(self.root)\n",
    "        self.root1.title(\"About\")\n",
    "        self.root1.protocol('WM_DELETE_WINDOW', self.destructor1)\n",
    "        self.root1.geometry(\"900x900\")\n",
    "        \n",
    "        # img = cv2.imread(\"Pictures/sir.jpg\", 1)\n",
    "        # # img = cv2.resize(img, (300, 300))\n",
    "        # cv2.imwrite(\"Pictures/sir.png\", img)\n",
    "        # return \n",
    "        \n",
    "        self.tx = tk.Label(self.root1)\n",
    "        self.tx.place(x = 330,y = 20)\n",
    "        self.tx.config(text = \"Efforts By\", fg=\"red\", font = (\"Courier\",30,\"bold\"))\n",
    "\n",
    "        self.photo1 = tk.PhotoImage(file='Pictures/ravi.png')\n",
    "        self.w1 = tk.Label(self.root1, image = self.photo1)\n",
    "        self.w1.place(x = 20, y = 105)\n",
    "        self.tx6 = tk.Label(self.root1)\n",
    "        self.tx6.place(x = 20,y = 250)\n",
    "        self.tx6.config(text = \"RC\\nIIT2016141\", font = (\"Courier\",15,\"bold\"))\n",
    "\n",
    "        self.photo2 = tk.PhotoImage(file='Pictures/nitin.png')\n",
    "        self.w2 = tk.Label(self.root1, image = self.photo2)\n",
    "        self.w2.place(x = 200, y = 105)\n",
    "        self.tx2 = tk.Label(self.root1)\n",
    "        self.tx2.place(x = 200,y = 250)\n",
    "        self.tx2.config(text = \"Nitin\\nIIT2016132\", font = (\"Courier\",15,\"bold\"))\n",
    "\n",
    "        \n",
    "        self.photo3 = tk.PhotoImage(file='Pictures/luv.png')\n",
    "        self.w3 = tk.Label(self.root1, image = self.photo3)\n",
    "        self.w3.place(x = 380, y = 105)\n",
    "        self.tx3 = tk.Label(self.root1)\n",
    "        self.tx3.place(x = 380,y = 250)\n",
    "        self.tx3.config(text = \"Luv\\nIIT2016085\", font = (\"Courier\",15,\"bold\"))\n",
    "\n",
    "        self.photo4 = tk.PhotoImage(file='Pictures/sheldon.png')\n",
    "        self.w4 = tk.Label(self.root1, image = self.photo4)\n",
    "        self.w4.place(x = 560, y = 105)\n",
    "        self.tx4 = tk.Label(self.root1)\n",
    "        self.tx4.place(x = 560,y = 250)\n",
    "        self.tx4.config(text = \"Sheldon\\nIIT2016137\", font = (\"Courier\",15,\"bold\"))\n",
    "        \n",
    "        self.photo5 = tk.PhotoImage(file='Pictures/sid.png')\n",
    "        self.w5 = tk.Label(self.root1, image = self.photo5)\n",
    "        self.w5.place(x = 740, y = 105)\n",
    "        self.tx5 = tk.Label(self.root1)\n",
    "        self.tx5.place(x = 740,y = 250)\n",
    "        self.tx5.config(text = \"Siddhant\\nIIT2016069\", font = (\"Courier\",15,\"bold\"))\n",
    "        \n",
    "        self.tx7 = tk.Label(self.root1)\n",
    "        self.tx7.place(x = 170,y = 360)\n",
    "        self.tx7.config(text = \"Under the supervision of\", fg=\"red\", font = (\"Courier\",30,\"bold\"))\n",
    "\n",
    "        self.photo6 = tk.PhotoImage(file='Pictures/sir.png')\n",
    "        self.w6 = tk.Label(self.root1, image = self.photo6)\n",
    "        self.w6.place(x = 350, y = 420)\n",
    "        self.tx6 = tk.Label(self.root1)\n",
    "        self.tx6.place(x = 230,y = 670)\n",
    "        self.tx6.config(text = \"Dr. Vrijendra Singh\", font = (\"Courier\",30,\"bold\"))\n",
    "\n",
    "print(\"Starting Application...\")\n",
    "pba = Application()\n",
    "pba.root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
