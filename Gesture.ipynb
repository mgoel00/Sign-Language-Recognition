{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "num_frames = 0\n",
    "element = 15\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            #cv2.imshow(\"Sign Detection\",frame_copy)\n",
    "         \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        \n",
    "        # Checking if hand is actually detected by counting number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                #cv2.imwrite(r\"D:\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" + str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                cv2.imwrite(\"dataset/train/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            elif num_imgs_taken > 300 and num_imgs_taken <= 400:\n",
    "                cv2.imwrite(\"dataset/test/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"Hand Sign Recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3010 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "train_path = 'dataset/train'\n",
    "test_path = 'dataset/test'\n",
    "\n",
    "train_batches = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, \n",
    "                                                                                             target_size=(64,64),\n",
    "                                                                                             class_mode='categorical', \n",
    "                                                                                             batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, \n",
    "                                                                                             target_size=(64,64), \n",
    "                                                                                             class_mode='categorical', \n",
    "                                                                                             batch_size=10, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 414,346\n",
      "Trainable params: 414,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4c870a4fe995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, checkpoint])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# For getting next batch of imgs...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# For getting next batch of imgs...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop], validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "model.save('best_model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(r\"best_model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {0:'One',1:'Ten',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
