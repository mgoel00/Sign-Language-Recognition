{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "num_frames = 0\n",
    "element = 'A'\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            #cv2.imshow(\"Sign Detection\",frame_copy)\n",
    "         \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        \n",
    "        # Checking if hand is actually detected by counting number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                #cv2.imwrite(r\"D:\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" + str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                cv2.imwrite(\"data2/train/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            elif num_imgs_taken > 300 and num_imgs_taken <= 400:\n",
    "                cv2.imwrite(\"data2/test/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "#             else:\n",
    "#                 break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"Hand Sign Recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 61, 61, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               3686528   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 96)                12384     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 3,716,443\n",
      "Trainable params: 3,716,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 12845 images belonging to 27 classes.\n",
      "Found 4268 images belonging to 27 classes.\n",
      "Epoch 1/5\n",
      "12841/12841 [==============================] - 2764s 215ms/step - loss: 0.4991 - acc: 0.8404 - val_loss: 0.0062 - val_acc: 0.9984\n",
      "Epoch 2/5\n",
      "12841/12841 [==============================] - 2214s 172ms/step - loss: 0.1196 - acc: 0.9634 - val_loss: 0.0042 - val_acc: 0.9988\n",
      "Epoch 3/5\n",
      "12841/12841 [==============================] - 2580s 201ms/step - loss: 0.0872 - acc: 0.9738 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "12841/12841 [==============================] - 2801s 218ms/step - loss: 0.0686 - acc: 0.9800 - val_loss: 0.0034 - val_acc: 0.9986\n",
      "Epoch 5/5\n",
      "12841/12841 [==============================] - 2707s 211ms/step - loss: 0.0612 - acc: 0.9829 - val_loss: 0.0035 - val_acc: 0.9991\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout\n",
    "import os\n",
    "\n",
    "sz = 128\n",
    "# Step 1 - Building the CNN\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "#classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=96, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
    "\n",
    "\n",
    "# Step 2 - Preparing the train/test data and training the model\n",
    "classifier.summary()\n",
    "# Code copied from - https://keras.io/preprocessing/image/\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/train',\n",
    "                                                 target_size=(sz, sz),\n",
    "                                                 batch_size=10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test',\n",
    "                                            target_size=(sz , sz),\n",
    "                                            batch_size=10,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical') \n",
    "\n",
    "batch_size = 32\n",
    "training_history = classifier.fit_generator(training_set,\n",
    "                                            steps_per_epoch=12841, # No of images in training set\n",
    "                                            epochs=5,\n",
    "                                            validation_data=test_set,\n",
    "                                            validation_steps=4268)# No of images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Char_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "from string import ascii_uppercase\n",
    "\n",
    "\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "prediction={}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "model = keras.models.load_model(\"Char_model.h5\")\n",
    "minValue = 25\n",
    "\n",
    "preds = ['blank']\n",
    "for i in ascii_uppercase:\n",
    "    preds.append(i)\n",
    "\n",
    "preds[0],preds[1] = preds[1],preds[0]\n",
    "\n",
    "\n",
    "num_frames = 0\n",
    "temp_res = []\n",
    "final_res = \"\"\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    # Simulating mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2.rectangle(frame, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)    # Extracting the ROI\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "#   roi = cv2.resize(roi, (128,128))\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "    # #blur = cv2.bilateralFilter(roi,9,75,75)\n",
    "\n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    test_image = cv2.resize(test_image, (128,128) )\n",
    "    \n",
    "    test_predict = test_image.reshape(1,128,128,1)\n",
    "    \n",
    "    if num_frames<30:\n",
    "        num_frames += 1\n",
    "        res = np.argmax(model.predict(test_predict, 1, verbose = 0), axis=1)\n",
    "        temp_res.append(preds[res[0]])\n",
    "    \n",
    "    if num_frames >=30:\n",
    "        num_frames = 0\n",
    "        final_res = max(set(temp_res), key = temp_res.count)\n",
    "        temp_res = []\n",
    "           \n",
    "\n",
    "    test_image = cv2.resize(test_image, (256,256) )\n",
    "#     cv2.putText(test_image, final_res , (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow(\"test\", test_image)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Application...\n",
      "Loaded model from disk\n",
      "Closing Application...\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import operator\n",
    "import time\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import hunspell\n",
    "from string import ascii_uppercase\n",
    "from pynput.keyboard import Key, Listener\n",
    "import keyboard\n",
    "\n",
    "message = [\" \"]\n",
    "\n",
    "class Application:\n",
    "    def __init__(self):\n",
    "        self.vs = cv2.VideoCapture(0)\n",
    "        self.current_image = None\n",
    "        self.current_image2 = None   \n",
    "        self.ct = {}\n",
    "        self.ct['blank'] = 0\n",
    "        self.blank_flag = 0\n",
    "        for i in ascii_uppercase:\n",
    "            self.ct[i] = 0\n",
    "        print(\"Loaded model from disk\")\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Sign language Detection\")\n",
    "        self.root.protocol('WM_DELETE_WINDOW', self.destructor)\n",
    "        self.root.geometry(\"950x900\")\n",
    "        self.panel = tk.Label(self.root)\n",
    "        self.panel.place(x = 135, y = 10, width = 640, height = 640)\n",
    "        self.panel2 = tk.Label(self.root) # initialize image panel\n",
    "        self.panel2.place(x = 460, y = 95, width = 310, height = 310)\n",
    "        \n",
    "        self.T = tk.Label(self.root)\n",
    "        self.T.place(x=120,y = 17)\n",
    "        self.T.config(text = \"Sign Language Detection\",font=(\"Helvetica\",40,\"bold italic\"))\n",
    "        self.panel3 = tk.Label(self.root) # Current SYmbol\n",
    "        self.panel3.place(x = 500,y=640)\n",
    "\n",
    "        self.T1 = tk.Label(self.root)\n",
    "        self.T1.place(x = 10,y = 640)\n",
    "        self.T1.config(text=\"Prediction :\",font=(\"Helvetica\",40,\"bold\"))\n",
    "        self.panel4 = tk.Label(self.root) # Word\n",
    "        self.panel4.place(x = 220,y=700)\n",
    "        self.T2 = tk.Label(self.root)\n",
    "        self.T2.place(x = 10,y = 700)\n",
    "        self.T2.config(text =\"Word :\",font=(\"Helvetica\",40,\"bold\"))\n",
    "        self.panel5 = tk.Label(self.root) # Sentence\n",
    "        self.panel5.place(x = 350,y=760)\n",
    "        self.T3 = tk.Label(self.root)\n",
    "        self.T3.place(x = 10,y = 760)\n",
    "        self.T3.config(text =\"Message :\",font=(\"Helvetica\",40,\"bold\"))\n",
    "\n",
    "        self.str=\"\"\n",
    "        self.word=\"\"\n",
    "\n",
    "        self.current_symbol=\"em\"\n",
    "        self.photo=\"Empty\"\n",
    "        self.video_loop()\n",
    "\n",
    "    def video_loop(self):\n",
    "        ok, frame = self.vs.read()\n",
    "        if ok:\n",
    "            cv2image = cv2.flip(frame, 1)\n",
    "            x1 = int(0.5*frame.shape[1])\n",
    "            y1 = 10\n",
    "            x2 = frame.shape[1]-10\n",
    "            y2 = int(0.5*frame.shape[1])\n",
    "            cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)\n",
    "            cv2image = cv2.cvtColor(cv2image, cv2.COLOR_BGR2RGBA)\n",
    "            self.current_image = Image.fromarray(cv2image)\n",
    "            imgtk = ImageTk.PhotoImage(image=self.current_image)\n",
    "            self.panel.imgtk = imgtk\n",
    "            self.panel.config(image=imgtk)\n",
    "            cv2image = cv2image[y1:y2, x1:x2]\n",
    "            gray = cv2.cvtColor(cv2image, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "            th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "            ret, res = cv2.threshold(th3, 70, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "#             self.predict(res)\n",
    "            self.current_image2 = Image.fromarray(res)\n",
    "            imgtk = ImageTk.PhotoImage(image=self.current_image2)\n",
    "            self.panel2.imgtk = imgtk\n",
    "            self.panel2.config(image=imgtk)\n",
    "            self.panel3.config(text=self.current_symbol,font=(\"courier\",50))\n",
    "            self.panel4.config(text=self.word,font=(\"courier\",40))\n",
    "            self.panel5.config(text=self.str,font=(\"courier\",40))\n",
    "\n",
    "        if keyboard.read_key() == None:\n",
    "            self.current_symbol = \"blank\"\n",
    "        else:\n",
    "            self.current_symbol = keyboard.read_key()\n",
    "            \n",
    "        if self.current_symbol == '1':\n",
    "            self.current_symbol = \"blank\"\n",
    "        if message[-1] != self.current_symbol:\n",
    "            if self.current_symbol == 'blank':\n",
    "                self.str += self.word.strip()\n",
    "                if len(self.word.strip()) > 0:\n",
    "                    self.str+=\" \"\n",
    "                self.str.strip()\n",
    "                message.clear()\n",
    "                self.word = \"\"\n",
    "                message.append(\" \")\n",
    "\n",
    "            else:\n",
    "                message.append(self.current_symbol)\n",
    "        \n",
    "        self.word = \"\".join(message).strip()\n",
    "        \n",
    "        self.root.after(30, self.video_loop)\n",
    "        \n",
    "    def destructor(self):\n",
    "        print(\"Closing Application...\")\n",
    "        self.root.destroy()\n",
    "        self.vs.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def destructor1(self):\n",
    "        print(\"Closing Application...\")\n",
    "        self.root1.destroy()\n",
    "\n",
    "print(\"Starting Application...\")\n",
    "pba = Application()\n",
    "pba.root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
