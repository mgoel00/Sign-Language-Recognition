{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "num_frames = 0\n",
    "element = 'A'\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            #cv2.imshow(\"Sign Detection\",frame_copy)\n",
    "         \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        \n",
    "        # Checking if hand is actually detected by counting number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            #cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                #cv2.imwrite(r\"D:\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" + str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                cv2.imwrite(\"dataset/train/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            elif num_imgs_taken > 300 and num_imgs_taken <= 400:\n",
    "                cv2.imwrite(\"dataset/test/\" + str(element) + \"/\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"Hand Sign Recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# train_path = 'dataset/train'\n",
    "# test_path = 'dataset/test'\n",
    "\n",
    "# train_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, \n",
    "#                                                                                              target_size=(64,64),\n",
    "#                                                                                              class_mode='categorical', \n",
    "#                                                                                              batch_size=10,shuffle=True)\n",
    "# test_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, \n",
    "#                                                                                              target_size=(64,64), \n",
    "#                                                                                              class_mode='categorical', \n",
    "#                                                                                              batch_size=10, shuffle=True)\n",
    "\n",
    "# imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "# #Plotting the images...\n",
    "# def plotImages(images_arr):\n",
    "#     fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "#     axes = axes.flatten()\n",
    "#     for img, ax in zip( images_arr, axes):\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         ax.imshow(img)\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 31, 31, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                460864    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 471,658\n",
      "Trainable params: 471,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(128,activation =\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# # model.add(Dense(256,activation =\"relu\"))\n",
    "# # model.add(Dropout(0.5))\n",
    "model.add(Dense(10,activation =\"softmax\"))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "376/376 [==============================] - 23s 62ms/step - loss: 14.3687 - acc: 0.1080 - val_loss: 12.9074 - val_acc: 0.1992\n",
      "Epoch 2/50\n",
      "376/376 [==============================] - 19s 49ms/step - loss: 14.4449 - acc: 0.1035 - val_loss: 14.5192 - val_acc: 0.0992\n",
      "Epoch 3/50\n",
      "376/376 [==============================] - 20s 53ms/step - loss: 14.3085 - acc: 0.1120 - val_loss: 12.9203 - val_acc: 0.1984\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 4/50\n",
      "376/376 [==============================] - 17s 45ms/step - loss: 12.9243 - acc: 0.1981 - val_loss: 12.8042 - val_acc: 0.2056\n",
      "Epoch 5/50\n",
      "376/376 [==============================] - 19s 50ms/step - loss: 12.9160 - acc: 0.1987 - val_loss: 12.8945 - val_acc: 0.2000\n",
      "Epoch 6/50\n",
      "376/376 [==============================] - 17s 45ms/step - loss: 12.9271 - acc: 0.1979 - val_loss: 12.8816 - val_acc: 0.2008\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 7/50\n",
      "376/376 [==============================] - 20s 52ms/step - loss: 12.8962 - acc: 0.1997 - val_loss: 12.9203 - val_acc: 0.1984\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"sign_model.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 3010\n",
    "nb_validation_samples = 1000\n",
    "batch_size=8\n",
    "\n",
    "history2 = model.fit_generator(train_batches, epochs=50,\n",
    "                               callbacks=[reduce_lr, earlystop],\n",
    "                               steps_per_epoch = nb_train_samples // batch_size,\n",
    "                               validation_data = test_batches,\n",
    "                               validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-546fb37f6802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreduce_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lr, momentum, decay, nesterov, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     def __init__(self, lr=0.01, momentum=0., decay=0.,\n\u001b[0;32m    172\u001b[0m                  nesterov=False, **kwargs):\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'iterations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 raise TypeError('Unexpected keyword argument '\n\u001b[1;32m---> 79\u001b[1;33m                                 'passed to optimizer: ' + str(k))\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: learning_rate"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 61, 61, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               3686528   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 96)                12384     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 3,716,443\n",
      "Trainable params: 3,716,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 12845 images belonging to 27 classes.\n",
      "Found 4268 images belonging to 27 classes.\n",
      "Epoch 1/5\n",
      "12841/12841 [==============================] - 2764s 215ms/step - loss: 0.4991 - acc: 0.8404 - val_loss: 0.0062 - val_acc: 0.9984\n",
      "Epoch 2/5\n",
      "12841/12841 [==============================] - 2214s 172ms/step - loss: 0.1196 - acc: 0.9634 - val_loss: 0.0042 - val_acc: 0.9988\n",
      "Epoch 3/5\n",
      "12841/12841 [==============================] - 2580s 201ms/step - loss: 0.0872 - acc: 0.9738 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "12841/12841 [==============================] - 2801s 218ms/step - loss: 0.0686 - acc: 0.9800 - val_loss: 0.0034 - val_acc: 0.9986\n",
      "Epoch 5/5\n",
      "12841/12841 [==============================] - 2707s 211ms/step - loss: 0.0612 - acc: 0.9829 - val_loss: 0.0035 - val_acc: 0.9991\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout\n",
    "import os\n",
    "\n",
    "sz = 128\n",
    "# Step 1 - Building the CNN\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "#classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=96, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
    "\n",
    "\n",
    "# Step 2 - Preparing the train/test data and training the model\n",
    "classifier.summary()\n",
    "# Code copied from - https://keras.io/preprocessing/image/\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('data2/train',\n",
    "                                                 target_size=(sz, sz),\n",
    "                                                 batch_size=10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('data2/test',\n",
    "                                            target_size=(sz , sz),\n",
    "                                            batch_size=10,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical') \n",
    "\n",
    "batch_size = 32\n",
    "training_history = classifier.fit_generator(training_set,\n",
    "                                            steps_per_epoch=12841, # No of images in training set\n",
    "                                            epochs=5,\n",
    "                                            validation_data=test_set,\n",
    "                                            validation_steps=4268)# No of images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a38049156b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Char_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('Char_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 14.50628662109375; acc of 10.000000149011612%\n",
      "{'val_loss': [14.519180572509766, 14.48049713897705, 14.480497093200684, 14.532075119018554, 14.54496954345703, 14.480497131347656], 'val_acc': [0.09920000195503235, 0.10160000199079514, 0.10160000187158584, 0.09840000200271606, 0.0976000018119812, 0.10160000216960907], 'loss': [14.200798349177584, 14.335183704153021, 14.261798237232451, 14.30264932044009, 14.401898211621223, 14.168607232418466], 'acc': [0.11622340652219793, 0.10957447013401604, 0.11462766179775304, 0.11196808734948331, 0.10558510837244227, 0.12074468316549951], 'lr': [0.001, 0.001, 0.001, 0.001, 0.000100000005, 0.000100000005]}\n",
      "loss of 12.894475936889648; acc of 20.000000298023224%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 414,346\n",
      "Trainable params: 414,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m         \u001b[0msqueeze\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'squeeze'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ca8ed2f69e57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iterations (per hundreds)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[0msqueeze\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1490\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'squeeze'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\goelm\\anaconda3\\envs\\cv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "# model.save('best_model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(\"best_model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "plt.plot(model.metrics_names[0],model.metrics_names[1])\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {0:'One',1:'Ten',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
